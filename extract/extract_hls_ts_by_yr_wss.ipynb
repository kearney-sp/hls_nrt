{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd19f52-3bab-45d0-984c-acef32851f96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rioxarray\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "from datetime import datetime, timedelta\n",
    "import dask\n",
    "from hpc_setup import launch_dask\n",
    "from hlsstack.hls_funcs import fetch\n",
    "from hlsstack.hls_funcs.masks import mask_hls, shp2mask, bolton_mask_xr, bolton_mask_np, atsa_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85741f-6fa6-4293-b4da-e1058145ec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch.setup_env(aws=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a6f69-feab-463f-bbf8-fdba8568d6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from importlib import reload\n",
    "#import sys\n",
    "#reload(sys.modules[\"ltar_jer_params_extract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90767c92-57b9-4463-9f2a-cbea3651dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '/project/cper_neon_aop/hls_nrt/params/co_wss_params/')\n",
    "\n",
    "from co_wss_params_extract import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c48120c-406d-4076-8dc5-f10b984e6cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if inPATH.split('.')[-1] == 'csv':\n",
    "    # load csv of ground data\n",
    "    df_aoi_txt = pd.read_csv(inPATH, parse_dates=[date_col])\n",
    "    if preprocess is not None:\n",
    "        df_aoi_txt = preprocess(df_aoi_txt)\n",
    "    # drop any IDs with missing coordinates or IDs\n",
    "    df_aoi_txt = df_aoi_txt.dropna(subset=[id_col, x_coord_col, y_coord_col])\n",
    "    df_aoi_txt = df_aoi_txt[df_aoi_txt['NAME'] == 'Weld']\n",
    "\n",
    "    # convert to GeoDataFrame using coordinates\n",
    "    gdf_aoi = gpd.GeoDataFrame(\n",
    "        df_aoi_txt, geometry=gpd.points_from_xy(df_aoi_txt[x_coord_col], df_aoi_txt[y_coord_col]))\n",
    "\n",
    "elif inPATH.split('.')[-1] == 'shp':\n",
    "    gdf_aoi = gpd.read_file(inPATH)\n",
    "    if preprocess is not None:\n",
    "        gdf_aoi = preprocess(gdf_aoi)\n",
    "# set the coordinate system\n",
    "if gdf_aoi.crs is None:\n",
    "    gdf_aoi = gdf_aoi.set_crs(epsg=input_epsg)\n",
    "# reproject to output coordinate system if different\n",
    "if input_epsg != output_epsg:\n",
    "    gdf_aoi = gdf_aoi.to_crs(epsg=output_epsg)\n",
    "# buffer the points to extract surrounding pixels later\n",
    "gdf_aoi.geometry = gdf_aoi.buffer(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960a4b1-7f7f-4030-85ae-da5c8dc9feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b2609c-c5c8-4676-99f3-4ed5d5f0b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_aoi = gdf_aoi.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de5e454-5f07-4f6f-8b23-1179f00be8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any polygons are empty\n",
    "any(gdf_aoi.area == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d0c4f-83b6-469a-bc5d-8a3eed622563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if multiple geometries exists for single ID, get average centroid and rebuffer\n",
    "if any(gdf_aoi.groupby(id_col).apply(lambda x: len(np.unique(x.geometry.astype('str'))) > 1)):\n",
    "    print('Averaging plots to centroid for multi-polygon plots:')\n",
    "    print()\n",
    "    # get average centroid and rebuffer\n",
    "    mean_polys = gdf_aoi.dissolve(by=id_col).centroid.buffer(buffer)\n",
    "    # rename geometry\n",
    "    mean_polys.name = 'geometry'\n",
    "    # convert to GeoDataFrame by resetting index\n",
    "    mean_polys = mean_polys.reset_index()\n",
    "    # overwrite geometry of original gdf with new geometry by matching ID\n",
    "    gdf_aoi.geometry = pd.merge(gdf_aoi,\n",
    "                            mean_polys,\n",
    "                            on=id_col,\n",
    "                            how='left')['geometry_y']\n",
    "else:\n",
    "    print('No multiple geometries found for unique ids')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12785132-3705-4541-8daf-63cc621e915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any polygon ID's overlap with other ID's within the same year\n",
    "gdf_overlapping = []\n",
    "gdf_non_overlapping = []\n",
    "for yr in gdf_aoi[date_col].dt.year.unique():\n",
    "    overlapping = []\n",
    "    non_overlapping = []\n",
    "    gdf_aoi_yr = gdf_aoi[gdf_aoi[date_col].dt.year == yr]\n",
    "    gdf_aoi_nodup_yr = gdf_aoi_yr.drop_duplicates(subset=[id_col])\n",
    "    geom_list = list(gdf_aoi_nodup_yr.geometry)\n",
    "    for n, p in enumerate(geom_list, 0):\n",
    "        if any(p.overlaps(g) or p.intersects(g) for g in [x for i,x in enumerate(geom_list) if i!=n]):\n",
    "            # Store the index from the original dataframe\n",
    "            overlapping.append(n)\n",
    "        else:\n",
    "            non_overlapping.append(n)\n",
    "    # Create a new dataframes and reset their indexes\n",
    "\n",
    "    if len(overlapping) > 0:\n",
    "        gdf_overlapping.append(gdf_aoi_nodup_yr.iloc[overlapping])\n",
    "    if len(non_overlapping) > 0:\n",
    "        gdf_non_overlapping.append(gdf_aoi_nodup_yr.iloc[non_overlapping])\n",
    "if len(gdf_overlapping) > 0:\n",
    "    gdf_overlapping = pd.concat(gdf_overlapping)\n",
    "    print(len(gdf_overlapping))\n",
    "else:\n",
    "    gdf_overlapping = None\n",
    "    print('No overlapping polygons detected.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393bd64-9bfb-4927-ad83-957207fcb309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview the plots\n",
    "# p1 = gdf_aoi.drop_duplicates(id_col).hvplot(tiles='EsriImagery', crs=gdf_aoi.crs.to_epsg(), \n",
    "#                color='red', alpha=0.5, hover=True, hover_cols=[id_col],\n",
    "#                frame_height=540, frame_width=800)\n",
    "# p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dff3a0-1d17-4a49-871f-ec8ed4c5362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.cluster.close()\n",
    "#client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda18ea-a593-4104-8979-c41d4ad0d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_aoi.geometry.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa746f1-0a8a-4f0d-a3e9-7335f92cfc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "interface_prefs = [\n",
    "        'ibp175s0',\n",
    "        'ibp59s0',\n",
    "        'enp24s0f0',\n",
    "        'ens7f0']\n",
    "interface = [x for x in interface_prefs if x in list(psutil.net_if_addrs().keys())][0]\n",
    "if len(interface) == 0:\n",
    "    print('ERROR: Preferred interfaces not found on node!')\n",
    "else:\n",
    "    print(interface)\n",
    "num_jobs=30\n",
    "client = launch_dask(cluster_loc=cluster_loc,\n",
    "                     num_jobs=num_jobs,\n",
    "                     mem_gb_per=4.0,\n",
    "                     partition='medium', \n",
    "                     duration='06:00:00',\n",
    "                     slurm_opts={'interface': interface},\n",
    "                    wait_timeout=400,\n",
    "                    debug=False)\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec653f19-c35d-4b3e-8eac-057217f0b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_atsa = True\n",
    "mask_bolton_by_pixel = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fee8ef-787d-484c-9b42-ba9723e9b158",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for yr in gdf_aoi[date_col].dt.year.unique():\n",
    "    print(yr)\n",
    "    gdf_aoi_sub_yr = gdf_aoi[gdf_aoi[date_col].dt.year == yr]\n",
    "    bbox_full = np.array(gdf_aoi_sub_yr.buffer(150).total_bounds)\n",
    "    if bbox_full[2] - bbox_full[0] > (30*400):\n",
    "        x_coords = np.arange(bbox_full[0], bbox_full[2], 30*400)\n",
    "        x_coords = np.append(x_coords, bbox_full[2])\n",
    "    else:\n",
    "        x_coords = [bbox_full[0], bbox_full[2]]\n",
    "    if bbox_full[3] - bbox_full[1] > (30*400):\n",
    "        y_coords = np.arange(bbox_full[1], bbox_full[3], 30*400)\n",
    "        y_coords = np.append(y_coords, bbox_full[3])\n",
    "    else:\n",
    "        y_coords = [bbox_full[1], bbox_full[3]]\n",
    "    \n",
    "    for xi, x in enumerate(x_coords[:-1]):\n",
    "        for yi, y in enumerate(y_coords[:-1]):\n",
    "            minx = x_coords[xi] - 45\n",
    "            maxx = x_coords[xi+1] + 45\n",
    "            miny = y_coords[yi] - 45\n",
    "            maxy = y_coords[yi+1] + 45\n",
    "            # expand window if small and running ATSA mask\n",
    "            if try_atsa:\n",
    "                if (maxx - minx) / 30 < 100:\n",
    "                    maxx = minx + (30*100)\n",
    "                if (maxy - miny) / 30 < 100:\n",
    "                    maxy = miny + (30*100)\n",
    "            print('Lower left: ', minx, ',', miny)\n",
    "            outPATH_tmp = os.path.join(outDIR,\n",
    "                                       'tmp', \n",
    "                                       re.sub('.csv', \n",
    "                                              '_' + '_'.join([str(yr), str(int(minx)), str(int(miny))]) + '.csv',\n",
    "                                              basename))\n",
    "            if os.path.exists(outPATH_tmp):\n",
    "                print('Extraction already complete for coords. Moving on.')\n",
    "                continue\n",
    "            elif len(gdf_aoi_sub_yr.cx[minx:maxx, miny:maxy]) == 0:\n",
    "                print('No plots in grid block. Moving on.')\n",
    "                continue\n",
    "            else:\n",
    "                idx_load_try = 0\n",
    "                max_load_try = 5\n",
    "                while idx_load_try < max_load_try:\n",
    "                    try: \n",
    "                        # make sure there are at least some workers before fetching data\n",
    "                        client.wait_for_workers(n_workers=int(num_jobs*0.1), timeout=200)\n",
    "                        \n",
    "                        start_date = gdf_aoi_sub_yr.cx[minx:maxx, miny:maxy][date_col].min() - timedelta(days=184)\n",
    "                        end_date = gdf_aoi_sub_yr.cx[minx:maxx, miny:maxy][date_col].max() + timedelta(days=184)\n",
    "                        # save the date range as a dictionary for fetching\n",
    "                        data_dict = {'date_range': [str(start_date.date()), \n",
    "                                                    str(end_date.date())]}\n",
    "                        hls_ds = fetch.get_hls(hls_data=data_dict,\n",
    "                                               bbox=np.array([minx, miny, maxx, maxy]), \n",
    "                                               stack_chunks=(1, -1, 450, 450),\n",
    "                                               debug=True,\n",
    "                                               proj_epsg=gdf_aoi.crs.to_epsg(),\n",
    "                                               lim=1000,\n",
    "                                               aws=False)\n",
    "                        hls_ds = hls_ds.assign_coords(tile_id = ('time', [x.split('.')[2] for x in hls_ds['id'].values]))\n",
    "                        hls_ds.sortby('time')\n",
    "                        \n",
    "                        #hls_ds\n",
    "                        \n",
    "                        hls_ds = hls_ds.load()\n",
    "                        idx_load_try = max_load_try\n",
    "                    except rio.errors.RasterioIOError:\n",
    "                        if idx_load_try == max_load_try:\n",
    "                            print('Fetching HLS failed for the max number of tries. Ending.')\n",
    "                            break\n",
    "                        else:\n",
    "                            print('Warning: error loading data. Retrying ' + str(idx_load_try+1) + ' of ' + str(max_load_try))\n",
    "                            # client.restart(wait_for_workers=False)\n",
    "                            client.wait_for_workers(n_workers=int(num_jobs*0.1), timeout=200)\n",
    "                            idx_load_try += 1   \n",
    "                    except RuntimeError:\n",
    "                        if idx_load_try == max_load_try:\n",
    "                            print('Fetching HLS failed for the max number of tries. Ending.')\n",
    "                            break\n",
    "                        else:\n",
    "                            print('Warning: error loading data. Retrying ' + str(idx_load_try+1) + ' of ' + str(max_load_try))\n",
    "                            # client.restart(wait_for_workers=False)\n",
    "                            client.wait_for_workers(n_workers=int(num_jobs*0.1), timeout=200)\n",
    "                            idx_load_try += 1   \n",
    "                        \n",
    "                # create a tile ID coordinate\n",
    "                hls_ds = hls_ds.assign_coords(tile_id = ('time', [x.split('.')[2] for x in hls_ds['id'].values]))\n",
    "                \n",
    "                # pick best image for any dates with duplicate images for the same tile\n",
    "                if len(hls_ds['time'].groupby('tile_id').apply(lambda x: x.drop_duplicates('time', False))) < len(hls_ds['time']):\n",
    "                    print('Dropping duplicate images for same tile.')\n",
    "                    hls_mask = mask_hls(hls_ds['FMASK'], mask_types=['all'])\n",
    "                    hls_ds['maskcov_pct'] = ((hls_mask != 0).sum(['y', 'x']) / hls_ds['FMASK'].isel(time=0).size * 100)#\n",
    "                    hls_ds = hls_ds.groupby('tile_id').apply(lambda x: x.sortby('maskcov_pct').drop_duplicates('time', keep='first')).sortby('time').compute()\n",
    "                \n",
    "                # merge and drop tile_id if multiple tiles exist, but don't overlap\n",
    "                if 'tile_id' in hls_ds.coords and \\\n",
    "                len(np.unique(hls_ds.tile_id.values)) > 1 and \\\n",
    "                len(np.unique(hls_ds.drop_duplicates(dim=['time', 'y', 'x'])['time'])) < len(np.unique(hls_ds['time'])):\n",
    "                    print('Multiple, overlapping tiles ids still exist for the same date. Need to figure out how to deal with this and keep mask intact.')\n",
    "                elif 'tile_id' in hls_ds.coords and len(np.unique(hls_ds.tile_id.values)) > 1:\n",
    "                    print('Dropping tile_id by taking mean across time dimension.')\n",
    "                    hls_ds = hls_ds.groupby('time').mean()\n",
    "                \n",
    "                display(hls_ds)\n",
    "                \n",
    "                # compute ATSA mask if possible\n",
    "                if try_atsa:\n",
    "                    print('masking out clouds and shadows detected by ATSA')\n",
    "                    if len(np.unique(hls_ds.drop_duplicates(dim=['time', 'y', 'x'])['time'])) < len(np.unique(hls_ds['time'])):\n",
    "                        print('Overlapping tiles found. Computing masks separately by tile id.')\n",
    "                        hls_atsa = hls_ds.groupby('tile_id').apply(lambda x: atsa_mask(x.where(\n",
    "                            x['BLUE'].notnull(), drop=True))).compute()\n",
    "                        hls_atsa = hls_atsa.transpose('time', 'y', 'x')\n",
    "                        mask_atsa = True\n",
    "                    else:\n",
    "                        hls_ds = hls_ds.reset_coords(drop=True)\n",
    "                        try:\n",
    "                            hls_atsa = atsa_mask(hls_ds).compute()\n",
    "                            mask_atsa = True\n",
    "                        except (ValueError, IndexError):\n",
    "                            print('WARNING: Could not compute ATSA cloud/shadow mask')\n",
    "                            mask_atsa = False\n",
    "                            pass\n",
    "                else:\n",
    "                    mask_atsa = False\n",
    "                \n",
    "                if mask_bolton_by_pixel:\n",
    "                    # compute the bolton mask\n",
    "                    hls_bolton_mask = bolton_mask(hls_ds).compute()\n",
    "                    hls_ds = xr.merge([hls_ds, hls_atsa], join='inner')\n",
    "                    hls_ds.where(hls_ds['BOLTON'] == 0, drop=True)\n",
    "                \n",
    "                # compute native HLS mask, including all aerosol flags\n",
    "                hls_mask = mask_hls(hls_ds['FMASK'], mask_types=['cirrus',\n",
    "                                                                 'cloud',\n",
    "                                                                 'cloud_adj',\n",
    "                                                                 'shadow', \n",
    "                                                                 'snow',\n",
    "                                                                 'water',\n",
    "                                                                 'high_aerosol'])\n",
    "                # mask using native HLS mask\n",
    "                hls_ds = hls_ds.where(hls_mask == 0)\n",
    "                # mask using ATSA mask, if available\n",
    "                \n",
    "                if mask_atsa:\n",
    "                    print('Applying ATSA mask to dataset.')\n",
    "                    # merge ATSA mask with HLS data\n",
    "                    hls_ds = xr.merge([hls_ds, hls_atsa], join='inner')\n",
    "                    hls_ds = hls_ds.where(hls_ds['ATSA'] == 1)\n",
    "                \n",
    "                # in case multiple tile_id's still exist, take the mean by pixel\n",
    "                if 'tile_id' in hls_ds.coords and \\\n",
    "                len(np.unique(hls_ds.tile_id.values)) > 1 and \\\n",
    "                len(np.unique(hls_ds.drop_duplicates(dim=['time', 'y', 'x'])['time'])) < len(np.unique(hls_ds['time'])):\n",
    "                    print('Multiple, overlapping tiles ids still exist, taking mean by pixel for each date')\n",
    "                    hls_ds = hls_ds.groupby('time').mean()\n",
    "\n",
    "                # lazy compute all vegetation indices\n",
    "                for vegidx in veg_dict:\n",
    "                    hls_ds[vegidx] = veg_dict[vegidx](hls_ds)\n",
    "\n",
    "                # subset overlapping polygons if they exist\n",
    "                if gdf_overlapping is not None:\n",
    "                    gdf_overlapping_yr = gdf_overlapping[gdf_overlapping[date_col].dt.year==yr].drop_duplicates(subset=[id_col])\n",
    "                    if len(gdf_overlapping_yr) > 0:\n",
    "                        print('subsetting overlapping polygons')\n",
    "                        gdf_aoi_sub_yr = gdf_aoi_sub_yr[~gdf_aoi_sub_yr[id_col].isin(gdf_overlapping_yr[id_col].unique())]\n",
    "                        df_list_overlapping = []\n",
    "                        for _, row in gdf_overlapping_yr.iterrows():\n",
    "                            # create an xarray mask from the plot\n",
    "                            mask_info = row.to_frame().transpose()[[id_col, 'geometry']].reset_index(drop=True).reset_index().rename(columns={'index': 'id'})\n",
    "                            mask_shp = [(row.geometry, row.id+1) for _, row in mask_info.iterrows()]\n",
    "                            plot_mask = shp2mask(shp=mask_shp, \n",
    "                                                 transform=hls_ds.rio.transform(), \n",
    "                                                 outshape=hls_ds['BLUE'].shape[1:], \n",
    "                                                 xr_object=hls_ds['BLUE'])\n",
    "                            mask_dict = {row.id+1: row[id_col] for _, row in mask_info.iterrows()}\n",
    "                            mask_dict[0] = 'UNK'\n",
    "                            plot_mask.values = np.array([mask_dict[i] for i in plot_mask.values.flatten()]).reshape(plot_mask.shape)\n",
    "                            \n",
    "                            # convert to dataframe for plot, if any pixels are NA, result is NA with skipna=False\n",
    "                            df_yr_sub_tmp = hls_ds.where(plot_mask == row[id_col], drop=True).mean(['y', 'x'], skipna=False).to_dataframe().reset_index()\n",
    "                            # add ID to dataframe\n",
    "                            df_yr_sub_tmp['Plot'] = row[id_col]\n",
    "                            # append to list\n",
    "                            df_list_overlapping.append(df_yr_sub_tmp)\n",
    "                        df_overlapping = pd.concat(df_list_overlapping)\n",
    "                    else:\n",
    "                        df_overlapping = None\n",
    "                else:\n",
    "                    df_overlapping = None\n",
    "                    \n",
    "                # create an xarray mask from the ground data\n",
    "                mask_info = gdf_aoi_sub_yr.drop_duplicates(\n",
    "                    subset=[id_col])[[id_col, 'geometry']].reset_index(drop=True).reset_index().rename(columns={'index': '_id'})\n",
    "                mask_shp = [(row.geometry, row._id+1) for _, row in mask_info.iterrows()]\n",
    "                plot_mask = shp2mask(shp=mask_shp, \n",
    "                                     transform=hls_ds.rio.transform(), \n",
    "                                     outshape=hls_ds['BLUE'].shape[1:], \n",
    "                                     xr_object=hls_ds['BLUE'])\n",
    "                mask_dict = {row._id+1: row[id_col] for _, row in mask_info.iterrows()}\n",
    "                mask_dict[0] = 'UNK'\n",
    "                plot_mask = np.array([mask_dict[i] for i in plot_mask.values.flatten()]).reshape(plot_mask.shape)\n",
    "                \n",
    "                # assign the plot id's to the xarray dataset\n",
    "                hls_ds = hls_ds.assign(Plot=(['y', 'x'], plot_mask)).chunk({'y': 50, 'x': 50})\n",
    "                hls_ds = hls_ds.set_coords('Plot')\n",
    "                \n",
    "                # mask out areas outside ground plots\n",
    "                hls_ds = hls_ds.where(hls_ds['Plot'] != 'UNK')\n",
    "                \n",
    "                # convert to dataframe at plot scale, if any pixels are NA, result is NA with skipna=False\n",
    "                df_yr_sub = hls_ds[list(veg_dict.keys()) + band_list].groupby(\n",
    "                    'Plot').mean('stacked_y_x', skipna=False).to_dataframe().reset_index()\n",
    "                # drop outside plots\n",
    "                df_yr_sub = df_yr_sub[df_yr_sub['Plot'] != 'UNK']\n",
    "\n",
    "                # add in overlapping polygon data if it exists\n",
    "                if df_overlapping is not None:\n",
    "                    df_yr_sub = pd.concat([df_yr_sub, df_overlapping])\n",
    "\n",
    "                # write to disk\n",
    "                df_yr_sub.to_csv(outPATH_tmp, index=False)\n",
    "\n",
    "                # delete datasets to free memory\n",
    "                del hls_ds, df_yr_sub, plot_mask, df_overlapping\n",
    "                \n",
    "                if mask_atsa:\n",
    "                    del hls_atsa\n",
    "        \n",
    "                # client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36005bfa-501f-475d-bffe-0358990a12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5758bb6d-9beb-4f86-8302-fe24e302ba72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3b92e-0b7c-4225-9e92-e5bf50252237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls_nrt_env",
   "language": "python",
   "name": "hls_nrt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
