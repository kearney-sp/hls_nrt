{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c509cb-1157-45d8-a40a-92c83bf6ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, concurrent.futures, time, warnings, os, re, pickle\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import glob\n",
    "import requests as r\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import rioxarray as riox\n",
    "import time\n",
    "import xarray as xr\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse,fromstring\n",
    "from pandas import to_datetime\n",
    "from rasterio.crs import CRS\n",
    "from datetime import datetime, timedelta\n",
    "from netrc import netrc\n",
    "from pyproj import Proj\n",
    "from src.hls_funcs import fetch\n",
    "from src.hls_funcs.masks import mask_hls, shp2mask, bolton_mask\n",
    "from src.hls_funcs.indices import ndvi_func\n",
    "from src.hls_funcs.smooth import smooth_xr, despike_ts_xr\n",
    "import cartopy.crs as ccrs\n",
    "from rasterio.plot import show\n",
    "from src.hls_funcs.predict import pred_bm, pred_bm_se, pred_cov\n",
    "import dask.diagnostics\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970df2e-308d-4f03-a8ff-98f9798bc158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wkDIR = '/project/cper_neon_aop/hls_nrt/'\n",
    "#outDIR = '/90daydata/cper_neon_aop/hls_nrt/' + prefix\n",
    "\n",
    "wkDIR = os.getcwd()\n",
    "outDIR_base = 'data/hls_nrt/'\n",
    "\n",
    "cluster_loc = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871d122-5747-4817-9f0f-148fbf226c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr = 2022\n",
    "\n",
    "#prefix = 'redtop' \n",
    "prefix = 'cper'\n",
    "\n",
    "if prefix == 'cper': \n",
    "    aoi_f = os.path.join('data/ground/cper_pastures_2017_clip.shp')\n",
    "    df_aoi = gpd.read_file(aoi_f)\n",
    "    subunit_name_old = 'Past_Name_'\n",
    "    subunit_name = 'Pasture'\n",
    "elif prefix == 'redtop':\n",
    "    from src.utils.convert import kmz_to_shp\n",
    "    df_aoi = kmz_to_shp('data/ground/RedTop_Boundary.kmz', 'data/ground/')\n",
    "    df_aoi = df_aoi.to_crs(epsg=32613)\n",
    "    subunit_name_old = None\n",
    "    subunit_name = None\n",
    "\n",
    "outDIR = os.path.join(outDIR_base, prefix)\n",
    "\n",
    "mod_bm = pickle.load(open('src/models/CPER_HLS_to_VOR_biomass_model_lr_simp.pk', 'rb'))\n",
    "mod_cov = pickle.load(open('src/models/CPER_HLS_to_LPI_cover_pls_binned_model.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78bdaa-616d-4eff-8d79-ee5c5791c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wkDIR)\n",
    "if cluster_loc == 'local':\n",
    "    print('   setting up Local cluster...')\n",
    "    from dask.distributed import LocalCluster, Client\n",
    "    import dask\n",
    "    aws=False\n",
    "    fetch.setup_env(aws=aws)\n",
    "    smth_chunks = {'y': 30, 'x': 30, 'time':-1}\n",
    "    cluster = LocalCluster(n_workers=8, threads_per_worker=2)\n",
    "    client = Client(cluster)\n",
    "    display(client)\n",
    "elif cluster_loc == 'coiled':\n",
    "    import coiled\n",
    "    aws=True\n",
    "    fetch.setup_env(aws=aws)\n",
    "    smth_chunks = {'y': 10, 'x': 10, 'time':-1}\n",
    "    s3_cred = fetch.setup_netrc(creds=['spkearney', '1mrChamu'], aws=aws)\n",
    "    coiled.create_software_environment(\n",
    "    name=\"hls_cog_coiled\",\n",
    "    conda=\"hls_cog_coiled_env.yaml\")\n",
    "    cluster = coiled.Cluster(\n",
    "        name=\"hls_cog_coiled\",\n",
    "        software=\"kearney-sp/hls_cog_coiled\",\n",
    "        n_workers=5,\n",
    "        worker_cpu=2,\n",
    "        scheduler_cpu=2,\n",
    "        backend_options={\"region\": \"us-west-2\"},\n",
    "        environ=dict(GDAL_DISABLE_READDIR_ON_OPEN='FALSE', \n",
    "                   #AWS_NO_SIGN_REQUEST='YES',\n",
    "                   GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "                   GDAL_SWATH_SIZE='200000000',\n",
    "                   VSI_CURL_CACHE_SIZE='200000000',\n",
    "                   CPL_VSIL_CURL_ALLOWED_EXTENSIONS='TIF',\n",
    "                   GDAL_HTTP_UNSAFESSL='YES',\n",
    "                   GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                   GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n",
    "                   AWS_REGION='us-west-2',\n",
    "                   AWS_SECRET_ACCESS_KEY=s3_cred['secretAccessKey'],\n",
    "                   AWS_ACCESS_KEY_ID=s3_cred['accessKeyId'],\n",
    "                   AWS_SESSION_TOKEN=s3_cred['sessionToken'])\n",
    "    )\n",
    "    client = Client(cluster)\n",
    "    display(client)\n",
    "elif cluster_loc == 'hpc':\n",
    "    from dask.distributed import LocalCluster, Client\n",
    "    import dask_jobqueue as jq\n",
    "    import dask\n",
    "    from jupyter_server import serverapp\n",
    "    \n",
    "    # get the server address for porting\n",
    "    try:\n",
    "        jupServer = [x for x in serverapp.list_running_servers()][0]\n",
    "    except IndexError:\n",
    "        # manually copy/paste the server address\n",
    "        jupServer = {'base_url': '/node/ceres19-compute-98-eth.scinet.local/17710/'}\n",
    "    print('   setting up moderate cluster on HPC...')\n",
    "    aws=False\n",
    "    fetch.setup_env(aws=aws)\n",
    "    smth_chunks = {'y': 10, 'x': 10, 'time':-1}\n",
    "    dask.config.set({'distributed.dashboard.link': jupServer['base_url'] + 'proxy/{port}/status'})\n",
    "    partition='short',#'short','debug', 'mem', 'mem-low',\n",
    "    num_processes = 4\n",
    "    num_threads_per_processes = 2\n",
    "    mem = 2.5*num_processes*num_threads_per_processes\n",
    "    n_cores_per_job = num_processes*num_threads_per_processes\n",
    "    clust = jq.SLURMCluster(queue=partition,\n",
    "                            processes=num_processes,\n",
    "                            cores=n_cores_per_job,\n",
    "                            memory=str(mem)+'GB',\n",
    "                            interface='ib0',\n",
    "                            #interface='enp24s0f0',\n",
    "                            local_directory='$TMPDIR',\n",
    "                            death_timeout=30,\n",
    "                            walltime='02:00:00',\n",
    "                            job_extra=[\"--output=/dev/null\",\"--error=/dev/null\"])\n",
    "    client=Client(clust)\n",
    "    #Scale Cluster \n",
    "    num_jobs=20\n",
    "    clust.scale(jobs=num_jobs)\n",
    "    try:\n",
    "        client.wait_for_workers(n_workers=num_jobs*num_processes, timeout=60)\n",
    "    except dask.distributed.TimeoutError as e:\n",
    "        print(str(num_jobs*num_processes) + ' workers not available. Continuing with available workers.')\n",
    "        #print(e)\n",
    "        pass\n",
    "    display(client)\n",
    "\n",
    "if not os.path.exists(outDIR):\n",
    "    os.mkdir(outDIR)\n",
    "\n",
    "if subunit_name_old is not None:\n",
    "    df_aoi = df_aoi.rename(columns={subunit_name_old: subunit_name})\n",
    "if subunit_name is not None:\n",
    "    df_aoi = df_aoi.dissolve(by=subunit_name).reset_index()\n",
    "\n",
    "start_date = str(yr - 1) + \"-11-01\"\n",
    "end_date = str(yr + 1) + \"-03-01\"\n",
    "\n",
    "# set the date range for analysis\n",
    "date_rng = pd.date_range(start=start_date, end=end_date)\n",
    "date_rng = date_rng[date_rng <= datetime.today()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3981d7f-3306-4b92-accd-5b5d06744aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t00 = time.time()\n",
    "nc_flist = list(filter(os.path.isfile, \n",
    "                    glob.glob(os.path.join(outDIR,\n",
    "                              prefix + \n",
    "                              '_hls_tmp/' + 'hls_ds_' + str(yr) + '*.nc'))))\n",
    "if len(nc_flist) > 0:\n",
    "    nc_flist.sort(key=lambda x: os.path.getctime(x))\n",
    "    nc_f = nc_flist[-1]\n",
    "else:\n",
    "    print('ERROR: No temp nc file found')\n",
    "\n",
    "hls_ds = xr.open_dataset(nc_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7435b-ae5a-4013-b84a-117efdb07688",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575be26-163b-43c1-a558-7e64d88b8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "print('   applying secondary cloud mask...')\n",
    "hls_ds = hls_ds.chunk({'time': -1, 'y': 100, 'x': 100})\n",
    "t1 = time.time()\n",
    "hls_bolton_mask = bolton_mask(hls_ds)\n",
    "# mask out scenes where < 75% of the region is cloud-free\n",
    "partial_mask = (hls_bolton_mask.count(dim=['y', 'x'])/np.product(hls_bolton_mask.shape[1:])) > 0.75\n",
    "hls_ds = hls_ds.where(hls_bolton_mask == 0)\n",
    "hls_ds = hls_ds.where(partial_mask).compute()\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "hls_ds = hls_ds.chunk({'time': 1, 'y': -1, 'x': -1})\n",
    "print('   computing NDVI for available dates...')\n",
    "t1 = time.time()\n",
    "# create ndvi array\n",
    "xr_ndvi = ndvi_func(hls_ds)#.compute()\n",
    "xr_ndvi.name = 'NDVI'\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "print('   computing biomass for available dates...')\n",
    "t1 = time.time()\n",
    "# create biomass array\n",
    "xr_bm = hls_ds.map_blocks(pred_bm, template=hls_ds['NIR1'],\n",
    "                          kwargs=dict(model=mod_bm)).where(hls_ds['NIR1'].notnull())#.compute()\n",
    "xr_bm.name = 'Biomass'\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "print('   computing biomass SE for available dates...')\n",
    "t1 = time.time()\n",
    "# create biomass array\n",
    "xr_bm_se = hls_ds.map_blocks(pred_bm_se, template=hls_ds['NIR1'],\n",
    "                          kwargs=dict(model=mod_bm)).where(hls_ds['NIR1'].notnull())#.compute()\n",
    "xr_bm_se.name = 'Biomass_SE'\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "print('   computing cover for available dates...')\n",
    "t1 = time.time()\n",
    "# create cover array\n",
    "xr_cov = pred_cov(hls_ds, model=mod_cov).where(hls_ds['NIR1'].notnull())#.compute()\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "print('   removing dates with > 75% cloud cover...')\n",
    "t1 = time.time()\n",
    "\n",
    "xr_ndvi = xr_ndvi.where(partial_mask, drop=True)\n",
    "xr_bm = xr_bm.where(partial_mask, drop=True)\n",
    "xr_bm_se = xr_bm_se.where(partial_mask, drop=True)\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ccfe49-4b40-40e5-b83a-61c097c9c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   creating daily template for output...')\n",
    "t1 = time.time()\n",
    "# create an output Dataset template with all dates\n",
    "dat_out = xr.Dataset(coords={'time': [x for x in date_rng if x not in xr_ndvi['time'].values],\n",
    "                             'x': xr_ndvi.x,\n",
    "                             'y': xr_ndvi.y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a936a5-6703-4711-97d6-f3e04f3a11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_nans = np.zeros((dat_out.dims['time'], \n",
    "                         dat_out.dims['y'], \n",
    "                         dat_out.dims['x'])) * np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202883b8-0b86-41fe-abfd-b3e4399192e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out = dat_out.assign(NDVI=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a01a3-856e-4913-bca2-28650a519462",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds = xr.concat([dat_out['NDVI'], xr_ndvi], dim='time').to_dataset()\n",
    "dat_out_ds = dat_out_ds.sortby('time')\n",
    "dat_out_ds = dat_out_ds.rio.write_crs(CRS.from_dict(init='epsg:32613'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a817ca-ebf4-4f93-aa36-1e674ea9aa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bba75c-4bf5-4ffc-a228-dfd702a471ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_mask = dat_out_ds['time'].astype(np.datetime64).dt.year == yr\n",
    "\n",
    "if not os.path.exists(os.path.join(outDIR, 'hls_ndvi/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_ndvi/'))\n",
    "if not os.path.exists(os.path.join(outDIR, 'hls_biomass/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_biomass/'))\n",
    "if not os.path.exists(os.path.join(outDIR, 'hls_cover/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_cover/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8590a2-78cf-46d3-8034-d062e080d5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds['NDVI_despike'] = despike_ts_xr(dat_out_ds['NDVI'].chunk(smth_chunks), \n",
    "                                           days_thresh=45,\n",
    "                                           dat_thresh=0.07, \n",
    "                                           mask_outliers=False,\n",
    "                                           iters=2,\n",
    "                                           dims=['time'])\n",
    "dat_out_ds['NDVI_smooth'] = smooth_xr(dat_out_ds['NDVI_despike'], \n",
    "                                      dims=['time'], \n",
    "                                      kwargs={'double': True, 'limit': 91})\n",
    "\n",
    "dat_out_ds.sel(time=yr_mask)['NDVI_smooth'].astype('float32').rio.reproject(\"EPSG:3857\").to_netcdf(\n",
    "    os.path.join(outDIR, 'hls_ndvi/' + prefix + '_hls_ndvi_' + str(yr) + '.nc'))\n",
    "\n",
    "dat_out_ds = dat_out_ds.drop_vars([k for k in dat_out_ds.data_vars.keys() if 'NDVI' in k])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343a870e-40c3-49ed-9307-af8f2dc166aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out = dat_out.rename({'NDVI': 'Biomass'})\n",
    "dat_out_ds['Biomass'] = xr.concat([dat_out['Biomass'], xr_bm], dim='time')\n",
    "dat_out_ds = dat_out_ds.sortby('time')\n",
    "\n",
    "dat_out_ds['Biomass_despike'] = despike_ts_xr(dat_out_ds['Biomass'].chunk(smth_chunks),\n",
    "                                              days_thresh=45,\n",
    "                                              dat_thresh=150, \n",
    "                                              mask_outliers=True,\n",
    "                                              z_thresh=5.0,\n",
    "                                              iters=2,\n",
    "                                              dims=['time'])\n",
    "dat_out_ds['Biomass_smooth'] = smooth_xr(dat_out_ds['Biomass_despike'], \n",
    "                                         dims=['time'],\n",
    "                                         kwargs={'double': True, 'limit': 91})\n",
    "\n",
    "dat_out_ds.sel(time=yr_mask)['Biomass_smooth'].astype('float32').rio.reproject(\"EPSG:3857\").to_netcdf(\n",
    "    os.path.join(outDIR, 'hls_biomass/' + prefix + '_hls_bm_' + str(yr) + '.nc'))\n",
    "\n",
    "dat_out_ds = dat_out_ds.drop_vars([k for k in dat_out_ds.data_vars.keys() if 'Biomass' in k and 'SE' not in k])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925d9c5-26d2-4077-9c2c-1c0a97419b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out = dat_out.rename({'Biomass': 'Biomass_SE'})\n",
    "dat_out_ds['Biomass_SE'] = xr.concat([dat_out['Biomass_SE'], xr_bm_se], dim='time')\n",
    "\n",
    "dat_out_ds['Biomass_SE_despike'] = despike_ts_xr(dat_out_ds['Biomass_SE'].chunk(smth_chunks),\n",
    "                                                 days_thresh=45,\n",
    "                                                 dat_thresh=dat_out_ds['Biomass_SE'].std('time').mean().values * 3.0, \n",
    "                                                 mask_outliers=True,\n",
    "                                                 z_thresh=5.0,\n",
    "                                                 iters=2,\n",
    "                                                 dims=['time'])\n",
    "dat_out_ds['Biomass_SE_smooth'] = smooth_xr(dat_out_ds['Biomass_SE_despike'], \n",
    "                                         dims=['time'],\n",
    "                                         kwargs={'double': True, 'limit': 91})\n",
    "\n",
    "dat_out_ds.sel(time=yr_mask)['Biomass_SE_smooth'].astype('float32').rio.reproject(\"EPSG:3857\").to_netcdf(\n",
    "    os.path.join(outDIR, 'hls_biomass/' + prefix + '_hls_bm_se_' + str(yr) + '.nc'))\n",
    "\n",
    "dat_out_ds = dat_out_ds.drop_vars([k for k in dat_out_ds.data_vars.keys() if 'Biomass_SE' in k])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98eb7ee-07a5-474a-a893-b38069512ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_prev = 'Biomass_SE'\n",
    "\n",
    "for c in ['BARE', 'SD', 'GREEN', 'LITT']:\n",
    "    print(c)\n",
    "    dat_out = dat_out.rename({c_prev: c})\n",
    "    dat_out_ds[c] = xr.concat([dat_out[c], xr_cov[c]], dim='time')\n",
    "    \n",
    "    dat_out_ds[c + '_despike'] = despike_ts_xr(dat_out_ds[c].chunk(smth_chunks),\n",
    "                                               days_thresh=45,\n",
    "                                               dat_thresh=0.3, \n",
    "                                               mask_outliers=True,\n",
    "                                               z_thresh=5.0,\n",
    "                                               iters=2,\n",
    "                                               dims=['time'])\n",
    "    dat_out_ds[c + '_smooth'] = smooth_xr(dat_out_ds[c + '_despike'], \n",
    "                                         dims=['time'],\n",
    "                                         kwargs={'double': True, 'limit': 91})\n",
    "\n",
    "    dat_out_ds.sel(time=yr_mask)[c + '_smooth'].astype('float32').rio.reproject(\"EPSG:3857\").to_netcdf(\n",
    "        os.path.join(outDIR, 'hls_cover/' + prefix + '_hls_' + c + '_' + str(yr) + '.nc'))\n",
    "    \n",
    "    dat_out_ds = dat_out_ds.drop_vars([k for k in dat_out_ds.data_vars.keys() if c in k])\n",
    "    gc.collect()\n",
    "    \n",
    "    c_prev = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbfc975-65a7-4e63-9475-529f7aa4e60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:121: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_post] - swir2_ts[idx_pre])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:121: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_post] - swir2_ts[idx_pre])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:121: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_post] - swir2_ts[idx_pre])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:103: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_pre] - swir2_ts[idx_pre2])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:121: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_post] - swir2_ts[idx_pre])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:121: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_post] - swir2_ts[idx_pre])\n",
      "/mnt/c/Users/TBGPEA-Sean/git_repos/hls_nrt/src/hls_funcs/masks.py:121: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  shadow_val = swir2_diff / (swir2_ts[idx_post] - swir2_ts[idx_pre])\n"
     ]
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bd93e-f371-4db1-a336-b6a046fff22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr_mask = dat_out_ds['time'].astype(np.datetime64).dt.year == yr\n",
    "out_profile = {'driver': 'GTiff',\n",
    "               'dtype': 'float32',\n",
    "               'nodata': -9999.,\n",
    "               'width': len(dat_out_ds.x),\n",
    "               'height': len(dat_out_ds.y),\n",
    "               'count': len(dat_out_ds.sel(time=yr_mask).time),\n",
    "               'crs': CRS.from_dict(init='epsg:32613'),\n",
    "               'transform': rio.Affine(30.0, 0.0, np.min(dat_out_ds.x).data,\n",
    "                                       0.0, -30.0, np.max(dat_out_ds.y).data),\n",
    "               'tiled': False}\n",
    "\n",
    "if not os.path.exists(os.path.join(outDIR, 'hls_ndvi/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_ndvi/'))\n",
    "if not os.path.exists(os.path.join(outDIR, 'hls_biomass/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_biomass/'))\n",
    "if not os.path.exists(os.path.join(outDIR, 'hls_cover/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_cover/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e118967-c1fc-4833-a54a-41e2f2c60f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds['NDVI_despike'] = despike_ts_xr(dat_out_ds['NDVI'].chunk(smth_chunks), \n",
    "                                           days_thresh=45,\n",
    "                                           dat_thresh=0.07, \n",
    "                                           mask_outliers=False,\n",
    "                                           iters=2,\n",
    "                                           dims=['time'])\n",
    "dat_out_ds['NDVI_smooth'] = smooth_xr(dat_out_ds['NDVI_despike'], \n",
    "                                      dims=['time'], \n",
    "                                      kwargs={'double': True, 'limit': 91})\n",
    "\n",
    "with rio.open(os.path.join(outDIR, 'hls_ndvi/' + prefix + '_hls_ndvi_' + str(yr) + '.tif'), \n",
    "              'w',\n",
    "              **out_profile) as dst:\n",
    "    dst.write(dat_out_ds.sel(time=yr_mask)['NDVI_smooth'].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38959b6-cb42-4138-863b-007a445890d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds.time.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeaf942-1e26-4f5d-a158-a929b12df50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out = dat_out.rename({'NDVI': 'Biomass'})\n",
    "dat_out_ds['Biomass'] = xr.concat([dat_out['Biomass'], xr_bm], dim='time')\n",
    "dat_out_ds = dat_out_ds.sortby('time')\n",
    "\n",
    "dat_out_ds['Biomass_despike'] = despike_ts_xr(dat_out_ds['Biomass'].chunk(smth_chunks),\n",
    "                                              days_thresh=45,\n",
    "                                              dat_thresh=150, \n",
    "                                              mask_outliers=True,\n",
    "                                              z_thresh=5.0,\n",
    "                                              iters=2,\n",
    "                                              dims=['time'])\n",
    "dat_out_ds['Biomass_smooth'] = smooth_xr(dat_out_ds['Biomass_despike'], \n",
    "                                         dims=['time'],\n",
    "                                         kwargs={'double': True, 'limit': 91})\n",
    "\n",
    "with rio.open(os.path.join(outDIR, 'hls_biomass/' + prefix + '_hls_bm_' + str(yr) + '.tif'), \n",
    "              'w',\n",
    "              **out_profile) as dst:\n",
    "    dst.write(dat_out_ds.sel(time=yr_mask)['Biomass_smooth'].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ad9b2-c0ca-4133-a88f-2470b27b16cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out = dat_out.rename({'Biomass': 'Biomass_SE'})\n",
    "dat_out_ds['Biomass_SE'] = xr.concat([dat_out['Biomass_SE'], xr_bm], dim='time')\n",
    "\n",
    "dat_out_ds['Biomass_SE_despike'] = despike_ts_xr(dat_out_ds['Biomass_SE'].chunk(smth_chunks),\n",
    "                                                 days_thresh=45,\n",
    "                                                 dat_thresh=dat_out_ds['Biomass_SE'].std('time').mean().values * 3.0, \n",
    "                                                 mask_outliers=True,\n",
    "                                                 z_thresh=5.0,\n",
    "                                                 iters=2,\n",
    "                                                 dims=['time'])\n",
    "dat_out_ds['Biomass_SE_smooth'] = smooth_xr(dat_out_ds['Biomass_SE_despike'], \n",
    "                                         dims=['time'],\n",
    "                                         kwargs={'double': True, 'limit': 91})\n",
    "\n",
    "with rio.open(os.path.join(outDIR, 'hls_biomass/' + prefix + '_hls_bm_se_' + str(yr) + '.tif'), \n",
    "              'w',\n",
    "              **out_profile) as dst:\n",
    "    dst.write(dat_out_ds.sel(time=yr_mask)['Biomass_SE_smooth'].astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d042d6-e3f3-44e8-891f-c5123c819465",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_prev = 'Biomass_SE'\n",
    "\n",
    "for c in ['BARE', 'SD', 'GREEN', 'LITT']:\n",
    "    print(c)\n",
    "    dat_out = dat_out.rename({c_prev: c})\n",
    "    dat_out_ds[c] = xr.concat([dat_out[c], xr_cov[c]], dim='time')\n",
    "    \n",
    "    dat_out_ds[c + '_despike'] = despike_ts_xr(dat_out_ds[c].chunk(smth_chunks),\n",
    "                                               days_thresh=45,\n",
    "                                               dat_thresh=0.3, \n",
    "                                               mask_outliers=True,\n",
    "                                               z_thresh=5.0,\n",
    "                                               iters=2,\n",
    "                                               dims=['time'])\n",
    "    dat_out_ds[c + '_smooth'] = smooth_xr(dat_out_ds[c + '_despike'], \n",
    "                                         dims=['time'],\n",
    "                                         kwargs={'double': True, 'limit': 91})\n",
    "\n",
    "    with rio.open(os.path.join(outDIR, 'hls_cover/' + prefix + '_hls_' + c + '_' + str(yr) + '.tif'), \n",
    "                  'w',\n",
    "                  **out_profile) as dst:\n",
    "        dst.write(dat_out_ds.sel(time=yr_mask)[c + '_smooth'].astype('float32'))\n",
    "    \n",
    "    c_prev = c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41af4cdf-7477-4bb5-a67c-be427d84ce7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe0346-2acf-4aaa-b6c3-9a4b388e15ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270b4ee-249c-4e64-943b-d21122ef2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_vars = xr.merge([xr_ndvi, xr_bm, xr_bm_se, xr_cov])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4ff96-c0e3-47c4-9528-c0c7082ef1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db557490-82ba-47cb-b8c9-291b2d729b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08475e1-6406-4856-87fb-c18fc484c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out = dat_out.assign(NDVI=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "dat_out = dat_out.assign(Biomass=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "dat_out = dat_out.assign(Biomass_SE=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "dat_out = dat_out.assign(BARE=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "dat_out = dat_out.assign(SD=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "dat_out = dat_out.assign(GREEN=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "dat_out = dat_out.assign(LITT=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cef268-e341-4ade-bff0-f392cc4fa02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds = xr.concat([dat_out['NDVI'], ds_vars['NDVI']], dim='time').to_dataset()\n",
    "print(dat_out_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c6b8b-d797-4e04-8c6c-db2e8b550dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds['Biomass'] = xr.concat([dat_out['Biomass'], ds_vars['Biomass']], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ee717-5ddb-4a90-a81b-a523e5aad8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds['Biomass_SE'] = xr.concat([dat_out['Biomass_SE'], ds_vars['Biomass_SE']], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088d4778-12e9-447e-a6da-adc23715805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dat_out_ds['BARE'] = xr.concat([dat_out['BARE'], ds_vars['BARE']], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f8a86-27f6-4990-82c7-c532173d8d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds['SD'] = xr.concat([dat_out['SD'], ds_vars['SD']], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551309a6-c1f1-496c-8268-da8a7c95f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds['GREEN'] = xr.concat([dat_out['GREEN'], ds_vars['GREEN']], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3155cd1d-26af-4847-8044-eb6f3df84ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds['LITT'] = xr.concat([dat_out['LITT'], ds_vars['LITT']], dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39244226-25ce-44f9-a348-616b7212d796",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat_out_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47543427-2a03-4dc6-a5b8-f0dd6509fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_out_ds = xr.concat([dat_out, ds_vars], dim='time')\n",
    "#dat_out_ds = dat_out_ds.sortby('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6cd919-a599-42b6-b4e3-6345b19ec3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dat_out_ds.nbytes*(9.31*10e-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80771df-5186-4d88-8247-f79094d176cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds_vars['BARE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04827b3c-9b62-4d20-9180-5bb55f570736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
