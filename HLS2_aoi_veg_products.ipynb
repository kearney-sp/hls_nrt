{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c509cb-1157-45d8-a40a-92c83bf6ec1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, concurrent.futures, time, warnings, os, re, pickle\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import glob\n",
    "import requests as r\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import rioxarray as riox\n",
    "import time\n",
    "import xarray as xr\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse,fromstring\n",
    "from pandas import to_datetime\n",
    "from rasterio.crs import CRS\n",
    "from datetime import datetime, timedelta\n",
    "from netrc import netrc\n",
    "from pyproj import Proj\n",
    "from src.hls_funcs import fetch\n",
    "from src.hls_funcs.masks import mask_hls, shp2mask, bolton_mask, atsa_mask\n",
    "from src.hls_funcs.indices import ndvi_func\n",
    "from src.hls_funcs.smooth import smooth_xr, despike_ts_xr\n",
    "import cartopy.crs as ccrs\n",
    "from rasterio.plot import show\n",
    "from src.hls_funcs.predict import pred_bm, pred_bm_se, pred_cov\n",
    "import dask.diagnostics\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1664c85f-851b-4a40-984b-5b46564ac551",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.update({'OMP_NUM_THREADS': '1',\n",
    "                  'MKL_NUM_THREADS': '1',\n",
    "                  'OPENBLAS_NUM_THREADS': '1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970df2e-308d-4f03-a8ff-98f9798bc158",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wkDIR = os.getcwd()\n",
    "outDIR_base = 'data/hls_nrt/'\n",
    "\n",
    "cluster_loc = 'hpc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871d122-5747-4817-9f0f-148fbf226c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr = 2013\n",
    "os.chdir(wkDIR)\n",
    "#prefix = 'redtop' \n",
    "prefix = 'cper'\n",
    "\n",
    "full_smooth = True\n",
    "\n",
    "if prefix == 'cper': \n",
    "    aoi_f = os.path.join('data/ground/cper_pastures_2017_clip.shp')\n",
    "    df_aoi = gpd.read_file(aoi_f)\n",
    "    subunit_name_old = 'Past_Name_'\n",
    "    subunit_name = 'Pasture'\n",
    "elif prefix == 'redtop':\n",
    "    from src.utils.convert import kmz_to_shp\n",
    "    df_aoi = kmz_to_shp('data/ground/RedTop_Boundary.kmz', 'data/ground/')\n",
    "    df_aoi = df_aoi.to_crs(epsg=32613)\n",
    "    subunit_name_old = None\n",
    "    subunit_name = None\n",
    "\n",
    "outDIR = os.path.join(outDIR_base, prefix)\n",
    "\n",
    "mod_bm = pickle.load(open('src/models/CPER_HLS_to_VOR_biomass_model_lr_simp.pk', 'rb'))\n",
    "mod_cov = pickle.load(open('src/models/CPER_HLS_to_LPI_cover_pls_binned_model.pk', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc78bdaa-616d-4eff-8d79-ee5c5791c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if cluster_loc == 'local':\n",
    "    os.chdir(wkDIR)\n",
    "    print('   setting up Local cluster...')\n",
    "    from dask.distributed import LocalCluster, Client\n",
    "    import dask\n",
    "    aws=False\n",
    "    fetch.setup_env(aws=aws)\n",
    "    smth_chunks = {'y': 30, 'x': 30, 'time':-1}\n",
    "    cluster = LocalCluster(n_workers=8, threads_per_worker=2)\n",
    "    client = Client(cluster)\n",
    "    display(client)\n",
    "elif cluster_loc == 'coiled':\n",
    "    import coiled\n",
    "    aws=True\n",
    "    fetch.setup_env(aws=aws)\n",
    "    smth_chunks = {'y': 10, 'x': 10, 'time':-1}\n",
    "    s3_cred = fetch.setup_netrc(creds=['spkearney', '1mrChamu'], aws=aws)\n",
    "    coiled.create_software_environment(\n",
    "    name=\"hls_cog_coiled\",\n",
    "    conda=\"hls_cog_coiled_env.yaml\")\n",
    "    cluster = coiled.Cluster(\n",
    "        name=\"hls_cog_coiled\",\n",
    "        software=\"kearney-sp/hls_cog_coiled\",\n",
    "        n_workers=5,\n",
    "        worker_cpu=2,\n",
    "        scheduler_cpu=2,\n",
    "        backend_options={\"region\": \"us-west-2\"},\n",
    "        environ=dict(GDAL_DISABLE_READDIR_ON_OPEN='FALSE', \n",
    "                   #AWS_NO_SIGN_REQUEST='YES',\n",
    "                   GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "                   GDAL_SWATH_SIZE='200000000',\n",
    "                   VSI_CURL_CACHE_SIZE='200000000',\n",
    "                   CPL_VSIL_CURL_ALLOWED_EXTENSIONS='TIF',\n",
    "                   GDAL_HTTP_UNSAFESSL='YES',\n",
    "                   GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                   GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n",
    "                   AWS_REGION='us-west-2',\n",
    "                   AWS_SECRET_ACCESS_KEY=s3_cred['secretAccessKey'],\n",
    "                   AWS_ACCESS_KEY_ID=s3_cred['accessKeyId'],\n",
    "                   AWS_SESSION_TOKEN=s3_cred['sessionToken'])\n",
    "    )\n",
    "    client = Client(cluster)\n",
    "    display(client)\n",
    "elif cluster_loc == 'hpc':\n",
    "    from dask.distributed import LocalCluster, Client\n",
    "    import dask_jobqueue as jq\n",
    "    import dask\n",
    "    from jupyter_server import serverapp\n",
    "    wkDIR = '/project/cper_neon_aop/hls_nrt/'\n",
    "    outDIR = '/90daydata/cper_neon_aop/hls_nrt/' + prefix\n",
    "    os.chdir(wkDIR)\n",
    "    # get the server address for porting\n",
    "    try:\n",
    "        jupServer = [x for x in serverapp.list_running_servers()][0]\n",
    "    except IndexError:\n",
    "        # manually copy/paste the server address\n",
    "        jupServer = {'base_url': '/node/ceres19-compute-98-eth.scinet.local/17710/'}\n",
    "    print('   setting up moderate cluster on HPC...')\n",
    "    aws=False\n",
    "    fetch.setup_env(aws=aws)\n",
    "    smth_chunks = {'y': 10, 'x': 10, 'time':-1}\n",
    "    dask.config.set({'distributed.dashboard.link': jupServer['base_url'] + 'proxy/{port}/status'})\n",
    "    partition='short',#'short','debug', 'mem', 'mem-low',\n",
    "    num_processes = 4\n",
    "    num_threads_per_processes = 2\n",
    "    mem = 2.5*num_processes*num_threads_per_processes\n",
    "    n_cores_per_job = num_processes*num_threads_per_processes\n",
    "    clust = jq.SLURMCluster(queue=partition,\n",
    "                            processes=num_processes,\n",
    "                            cores=n_cores_per_job,\n",
    "                            memory=str(mem)+'GB',\n",
    "                            interface='ib0',\n",
    "                            #interface='enp24s0f0',\n",
    "                            local_directory='$TMPDIR',\n",
    "                            death_timeout=30,\n",
    "                            walltime='02:00:00',\n",
    "                            job_extra=[\"--output=/dev/null\",\"--error=/dev/null\"])\n",
    "    client=Client(clust)\n",
    "    #Scale Cluster \n",
    "    num_jobs=20\n",
    "    clust.scale(jobs=num_jobs)\n",
    "    try:\n",
    "        client.wait_for_workers(n_workers=num_jobs*num_processes, timeout=60)\n",
    "    except dask.distributed.TimeoutError as e:\n",
    "        print(str(num_jobs*num_processes) + ' workers not available. Continuing with available workers.')\n",
    "        #print(e)\n",
    "        pass\n",
    "    display(client)\n",
    "\n",
    "if not os.path.exists(outDIR):\n",
    "    os.mkdir(outDIR)\n",
    "\n",
    "if subunit_name_old is not None:\n",
    "    df_aoi = df_aoi.rename(columns={subunit_name_old: subunit_name})\n",
    "if subunit_name is not None:\n",
    "    df_aoi = df_aoi.dissolve(by=subunit_name).reset_index()\n",
    "\n",
    "start_date = str(yr - 1) + \"-11-01\"\n",
    "end_date = str(yr + 1) + \"-03-01\"\n",
    "\n",
    "# set the date range for analysis\n",
    "date_rng = pd.date_range(start=start_date, end=end_date)\n",
    "date_rng = date_rng[date_rng <= datetime.today()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3981d7f-3306-4b92-accd-5b5d06744aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t00 = time.time()\n",
    "nc_flist = list(filter(os.path.isfile, \n",
    "                    glob.glob(os.path.join(outDIR,\n",
    "                              prefix + \n",
    "                              '_hls_tmp/' + 'hls_ds_' + str(yr) + '*.nc'))))\n",
    "if len(nc_flist) > 0:\n",
    "    nc_flist.sort(key=lambda x: os.path.getctime(x))\n",
    "    nc_f = nc_flist[-1]\n",
    "else:\n",
    "    print('ERROR: No temp nc file found')\n",
    "\n",
    "hls_ds = xr.open_dataset(nc_f).load()\n",
    "#hls_atsa = xr.open_dataset(re.sub('hls_ds_', 'hls_atsa_', nc_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7639fd6b-a6c4-48af-b306-7845a34d873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2a6f48-400a-4cc6-8394-a595da5cdd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(hls_ds['time'].groupby('tile_id').apply(lambda x: x.drop_duplicates('time', False))) < len(hls_ds['time']):\n",
    "    print('    reducing along id dimension to single observation for each date, keeping least-masked image')\n",
    "    print('NOTE: THIS SHOULD NOT BE NEEDED! GO BACK AND CHECK HLS2_aoi_yr_download.ipynb')\n",
    "    hls_ds = hls_ds.groupby('tile_id').apply(lambda x: x.sortby('maskcov_pct').drop_duplicates('time', keep='first')).sortby('time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a98b3e1-09c0-43c2-ad7a-84259b23d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hls_bolton_mask = bolton_mask(hls_ds).compute()\n",
    "#hls_mask_snow = mask_hls(hls_ds['FMASK'], mask_types=['snow'])\n",
    "#hls_mask = mask_hls(hls_ds['FMASK'], mask_types=['all'])\n",
    "\n",
    "#show((hls_bolton.sum(dim='time') / len(hls_mask.time)) > 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a265ed29-1677-49e6-8c31-fc0008c5302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(np.unique(hls_ds.tile_id)) > 1:\n",
    "    hls_atsa = hls_ds.groupby('tile_id').apply(lambda x: atsa_mask(x.where(\n",
    "        x['BLUE'].notnull(), drop=True))).compute()\n",
    "    hls_atsa = hls_atsa.transpose('time', 'y', 'x')\n",
    "else:\n",
    "    hls_ds = hls_ds.reset_coords(drop=True)\n",
    "    try:\n",
    "        hls_atsa = atsa_mask(hls_ds).compute()\n",
    "        mask_atsa = True\n",
    "    except ValueError:\n",
    "        print('WARNING: Could not compute ATSA cloud/shadow mask')\n",
    "        mask_atsa = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc298ba-4938-4349-a89b-e091e64082b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1489520f-6968-4503-afd7-a18a11fbf781",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds = xr.merge([hls_ds, hls_atsa], join='inner')\n",
    "#hls_mask = mask_hls(hls_ds['FMASK'], mask_types=['all'])\n",
    "hls_mask = mask_hls(hls_ds['FMASK'], mask_types=['cirrus',\n",
    "                                                'cloud',\n",
    "                                                'cloud_adj',\n",
    "                                                'shadow', \n",
    "                                                'snow',\n",
    "                                                'water'])\n",
    "#hls_mask = mask_hls(hls_ds['FMASK'], 'all')\n",
    "hls_ds = hls_ds.where(hls_mask == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251bb5a2-2a83-4225-94f7-acdf7e7608cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask_atsa:\n",
    "    print('masking out clouds and shadows detected by ATSA')\n",
    "    hls_ds = hls_ds.where(hls_ds['ATSA'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e35ac1-29a8-4452-b74a-d2ab45583694",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'tile_id' in hls_ds.coords and len(hls_ds.tile_id) > 1:\n",
    "    hls_ds = hls_ds.groupby('time').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80d44cd-a97d-4170-a533-321f6b483df1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "#print('   applying tertiary cloud mask...')\n",
    "#hls_ds = hls_ds.chunk({'time': -1, 'y': 100, 'x': 100})\n",
    "t1 = time.time()\n",
    "#hls_bolton_mask = bolton_mask(hls_ds).compute()\n",
    "#hls_ds = hls_ds.where(hls_bolton_mask == 0, drop=True)\n",
    "# mask out scenes where < 75% of the region is cloud-free\n",
    "partial_mask = (hls_ds['NIR1'].isnull().sum(dim=['y', 'x'])/np.product(hls_ds['NIR1'].shape[1:])) < 0.25\n",
    "partial_mask = partial_mask#.persist()\n",
    "hls_ds = hls_ds.where(partial_mask, drop=True).compute()\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328147e1-ae90-43a7-8a07-d635f5a1194c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2e7b26-7a29-4984-8b1e-e76e025485a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bba75c-4bf5-4ffc-a228-dfd702a471ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(outDIR, 'hls_ndvi/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_ndvi/'))\n",
    "if not os.path.exists(os.path.join(outDIR, 'hls_biomass/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_biomass/'))\n",
    "if not os.path.exists(os.path.join(outDIR, 'hls_cover/')):\n",
    "    os.mkdir(os.path.join(outDIR, 'hls_cover/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a92aa52-a270-4f9d-bb26-13c87780d21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds = hls_ds.chunk({'time': 1, 'y': -1, 'x': -1})\n",
    "print('   computing NDVI for available dates...')\n",
    "# create ndvi array\n",
    "xr_ndvi = ndvi_func(hls_ds)#.compute()\n",
    "xr_ndvi.name = 'NDVI'\n",
    "\n",
    "\n",
    "print('   creating daily template for output...')\n",
    "# reset date range to be within dataset\n",
    "date_rng = date_rng[date_rng <= xr_ndvi['time'].max().values]\n",
    "\n",
    "# create an output Dataset template with all dates\n",
    "dat_out = xr.Dataset(coords={'time': [x for x in date_rng if x not in xr_ndvi['time'].values],\n",
    "                             'x': xr_ndvi.x,\n",
    "                             'y': xr_ndvi.y})\n",
    "\n",
    "print('   interpolating and saving to disk')\n",
    "t1 = time.time()\n",
    "\n",
    "dat_out_nans = np.zeros((dat_out.dims['time'], \n",
    "                         dat_out.dims['y'], \n",
    "                         dat_out.dims['x'])) * np.nan\n",
    "\n",
    "dat_out = dat_out.assign(NDVI=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "\n",
    "dat_out_ds = xr.concat([dat_out['NDVI'], xr_ndvi], dim='time').to_dataset()\n",
    "dat_out_ds = dat_out_ds.sortby('time')\n",
    "dat_out_ds = dat_out_ds.rio.write_crs(CRS.from_dict(init='epsg:32613'))\n",
    "\n",
    "yr_mask = dat_out_ds['time'].astype(np.datetime64).dt.year == yr\n",
    "\n",
    "dat_out_ds = dat_out_ds.chunk({'time': -1, 'y': 20, 'x': 20})\n",
    "\n",
    "if full_smooth:\n",
    "    dat_out_da = despike_ts_xr(dat_out_ds['NDVI'],\n",
    "                               dat_thresh=0.07, \n",
    "                               mask_outliers=False,\n",
    "                               iters=2,\n",
    "                               dims=['time'])\n",
    "    dat_out_da = smooth_xr(dat_out_da, \n",
    "                           dims=['time'], \n",
    "                           kwargs={'double': True, 'limit': 91})\n",
    "else:\n",
    "    dat_out_da = dat_out_ds['NDVI'].interpolate_na('time').rolling(time=21, \n",
    "                                                                   center=True,\n",
    "                                                                   min_periods=1).mean()\n",
    "\n",
    "dat_out_da = dat_out_da.sel(time=yr_mask).astype('float32').rio.reproject(\"EPSG:3857\").compute()\n",
    "    \n",
    "dat_out_da.to_netcdf(\n",
    "    os.path.join(outDIR, 'hls_ndvi/' + prefix + '_hls_ndvi_' + str(yr) + '.nc'))\n",
    "\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "del dat_out_da, dat_out_ds, dat_out, dat_out_nans\n",
    "gc.collect()\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7ca2c0-2746-4980-b93a-109e7b16b5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   computing biomass for available dates...')\n",
    "# create biomass array\n",
    "xr_bm = hls_ds.map_blocks(pred_bm, template=hls_ds['NIR1'],\n",
    "                          kwargs=dict(model=mod_bm)).where(hls_ds['NIR1'].notnull())#.compute()\n",
    "xr_bm.name = 'Biomass'\n",
    "\n",
    "print('   creating daily template for output...')\n",
    "# reset date range to be within dataset\n",
    "date_rng = date_rng[date_rng <= xr_bm['time'].max().values]\n",
    "\n",
    "# create an output Dataset template with all dates\n",
    "dat_out = xr.Dataset(coords={'time': [x for x in date_rng if x not in xr_bm['time'].values],\n",
    "                             'x': xr_bm.x,\n",
    "                             'y': xr_bm.y})\n",
    "\n",
    "print('   interpolating and saving to disk')\n",
    "t1 = time.time()\n",
    "\n",
    "dat_out_nans = np.zeros((dat_out.dims['time'], \n",
    "                         dat_out.dims['y'], \n",
    "                         dat_out.dims['x'])) * np.nan\n",
    "\n",
    "dat_out = dat_out.assign(Biomass=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "\n",
    "dat_out_ds = xr.concat([dat_out['Biomass'], xr_bm], dim='time').to_dataset()\n",
    "dat_out_ds = dat_out_ds.sortby('time')\n",
    "dat_out_ds = dat_out_ds.rio.write_crs(CRS.from_dict(init='epsg:32613'))\n",
    "\n",
    "dat_out_ds['Biomass'] = xr.concat([dat_out['Biomass'], xr_bm], dim='time')\n",
    "dat_out_ds = dat_out_ds.sortby('time').chunk({'time': -1, 'y': 20, 'x': 20})\n",
    "\n",
    "if full_smooth:\n",
    "    dat_out_da = despike_ts_xr(dat_out_ds['Biomass'],\n",
    "                               dat_thresh=150, \n",
    "                               mask_outliers=True,\n",
    "                               z_thresh=5.0,\n",
    "                               iters=2,\n",
    "                               dims=['time'])\n",
    "    dat_out_da = smooth_xr(dat_out_da, \n",
    "                           dims=['time'], \n",
    "                           kwargs={'double': True, 'limit': 91})\n",
    "else:\n",
    "    dat_out_da = dat_out_ds['Biomass'].interpolate_na('time').rolling(time=21, \n",
    "                                                                      center=True,\n",
    "                                                                      min_periods=1).mean()\n",
    "\n",
    "dat_out_da = dat_out_da.sel(time=yr_mask).astype('float32').rio.reproject(\"EPSG:3857\").compute()\n",
    "\n",
    "dat_out_da.to_netcdf(\n",
    "    os.path.join(outDIR, 'hls_biomass/' + prefix + '_hls_bm_' + str(yr) + '.nc'))\n",
    "\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "del dat_out_da, dat_out_ds, dat_out, dat_out_nans\n",
    "gc.collect()\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bef894-85be-4ab9-914d-7791ea2912f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   computing biomass SE for available dates...')\n",
    "# create biomass SE array\n",
    "xr_bm_se = hls_ds.map_blocks(pred_bm_se, template=hls_ds['NIR1'],\n",
    "                          kwargs=dict(model=mod_bm)).where(hls_ds['NIR1'].notnull())#.compute()\n",
    "xr_bm_se.name = 'Biomass_SE'\n",
    "\n",
    "print('   creating daily template for output...')\n",
    "# reset date range to be within dataset\n",
    "date_rng = date_rng[date_rng <= xr_bm_se['time'].max().values]\n",
    "\n",
    "# create an output Dataset template with all dates\n",
    "dat_out = xr.Dataset(coords={'time': [x for x in date_rng if x not in xr_bm_se['time'].values],\n",
    "                             'x': xr_bm_se.x,\n",
    "                             'y': xr_bm_se.y})\n",
    "\n",
    "print('   interpolating and saving to disk')\n",
    "t1 = time.time()\n",
    "\n",
    "dat_out_nans = np.zeros((dat_out.dims['time'], \n",
    "                         dat_out.dims['y'], \n",
    "                         dat_out.dims['x'])) * np.nan\n",
    "\n",
    "dat_out = dat_out.assign(Biomass_SE=(['time', 'y', 'x'],\n",
    "                               dat_out_nans))\n",
    "\n",
    "dat_out_ds = xr.concat([dat_out['Biomass_SE'], xr_bm_se], dim='time').to_dataset()\n",
    "dat_out_ds = dat_out_ds.sortby('time')\n",
    "dat_out_ds = dat_out_ds.rio.write_crs(CRS.from_dict(init='epsg:32613'))\n",
    "\n",
    "dat_out_ds['Biomass_SE'] = xr.concat([dat_out['Biomass_SE'], xr_bm_se], dim='time')\n",
    "dat_out_ds = dat_out_ds.sortby('time').chunk({'time': -1, 'y': 20, 'x': 20})\n",
    "\n",
    "dat_out_da = dat_out_ds['Biomass_SE'].interpolate_na('time').rolling(time=21, \n",
    "                                                                     center=True,\n",
    "                                                                     min_periods=1).mean()\n",
    "\n",
    "dat_out_da = dat_out_da.sel(time=yr_mask).astype('float32').rio.reproject(\"EPSG:3857\").compute()\n",
    "\n",
    "dat_out_da.to_netcdf(\n",
    "    os.path.join(outDIR, 'hls_biomass/' + prefix + '_hls_bm_se_' + str(yr) + '.nc'))\n",
    "\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "del dat_out_da, dat_out_ds, dat_out, dat_out_nans\n",
    "gc.collect()\n",
    "client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e466f3-23ec-448d-9c18-0fdde0c7a0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('   computing cover for available dates...')\n",
    "# create cover array\n",
    "xr_cov = pred_cov(hls_ds, \n",
    "                  model=mod_cov).where(hls_ds['NIR1'].notnull()).compute()\n",
    "\n",
    "print('   creating daily template for output...')\n",
    "# reset date range to be within dataset\n",
    "date_rng = date_rng[date_rng <= xr_cov['time'].max().values]\n",
    "\n",
    "for c in ['BARE', 'SD', 'GREEN', 'LITT']:\n",
    "    print(c)\n",
    "    # create an output Dataset template with all dates\n",
    "    dat_out = xr.Dataset(coords={'time': [x for x in date_rng if x not in xr_cov['time'].values],\n",
    "                                 'x': xr_cov.x,\n",
    "                                 'y': xr_cov.y})\n",
    "\n",
    "    print('   interpolating and saving to disk')\n",
    "    t1 = time.time()\n",
    "\n",
    "    dat_out_nans = np.zeros((dat_out.dims['time'], \n",
    "                             dat_out.dims['y'], \n",
    "                             dat_out.dims['x'])) * np.nan\n",
    "\n",
    "    dat_out = dat_out.assign(TMP=(['time', 'y', 'x'],\n",
    "                                   dat_out_nans))\n",
    "    dat_out = dat_out.rename({'TMP': c})\n",
    "\n",
    "    dat_out_ds = xr.concat([dat_out[c], xr_cov[c]], dim='time').to_dataset()\n",
    "    dat_out_ds = dat_out_ds.sortby('time')\n",
    "    dat_out_ds = dat_out_ds.rio.write_crs(CRS.from_dict(init='epsg:32613'))\n",
    "\n",
    "    dat_out_ds[c] = xr.concat([dat_out[c], xr_cov[c]], dim='time')\n",
    "    dat_out_ds = dat_out_ds.sortby('time').chunk({'time': -1, 'y': 20, 'x': 20})\n",
    "    \n",
    "    if full_smooth:\n",
    "        dat_out_da = despike_ts_xr(dat_out_ds[c],\n",
    "                                   dat_thresh=0.15, \n",
    "                                   mask_outliers=False,\n",
    "                                   iters=2,\n",
    "                                   dims=['time'])\n",
    "        dat_out_da = smooth_xr(dat_out_da, \n",
    "                               dims=['time'], \n",
    "                               kwargs={'double': True, 'limit': 91})\n",
    "    else:\n",
    "        dat_out_da = dat_out_ds[c].interpolate_na('time').rolling(time=21, \n",
    "                                                                  center=True,\n",
    "                                                                  min_periods=1).mean()\n",
    "        \n",
    "    dat_out_da = dat_out_da.sel(time=yr_mask).astype('float32').rio.reproject(\"EPSG:3857\").compute()\n",
    "    \n",
    "    dat_out_da.to_netcdf(\n",
    "        os.path.join(outDIR, 'hls_cover/' + prefix + '_hls_' + c + '_' + str(yr) + '.nc'))\n",
    "\n",
    "    print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "    print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "    del dat_out_da, dat_out_ds, dat_out, dat_out_nans\n",
    "    gc.collect()\n",
    "    client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98242dd-b980-491d-8786-76bc1409b52c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126a4c14-5be0-4574-af83-16b440aeca82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls_nrt_env",
   "language": "python",
   "name": "hls_nrt_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
