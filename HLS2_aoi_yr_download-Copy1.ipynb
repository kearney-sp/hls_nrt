{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5eda9-22cf-42b5-82c1-4e91c20c540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask, concurrent.futures, time, warnings, os, re, pickle\n",
    "from osgeo import gdal\n",
    "import os\n",
    "import requests as r\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import rioxarray as riox\n",
    "import time\n",
    "import xarray as xr\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse,fromstring\n",
    "from pandas import to_datetime\n",
    "from rasterio.crs import CRS\n",
    "from datetime import datetime, timedelta\n",
    "from netrc import netrc\n",
    "from pyproj import Proj\n",
    "from src.hls_funcs import fetch\n",
    "from src.hls_funcs.masks import mask_hls, shp2mask, bolton_mask\n",
    "from src.hls_funcs.indices import ndvi_func\n",
    "from src.hls_funcs.smooth import smooth_xr, despike_ts_xr\n",
    "import cartopy.crs as ccrs\n",
    "from rasterio.plot import show\n",
    "from src.hls_funcs.predict import pred_bm, pred_bm_se, pred_cov\n",
    "import dask.diagnostics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62dd86-53e6-4bbc-861e-5dfd80e20388",
   "metadata": {},
   "outputs": [],
   "source": [
    "wkDIR = os.getcwd()\n",
    "outDIR_base = 'data/hls_nrt/'\n",
    "\n",
    "#wkDIR = '/project/cper_neon_aop/hls_nrt/'\n",
    "#outDIR_base = '/90daydata/cper_neon_aop/hls_nrt/'\n",
    "\n",
    "cluster_loc = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c07dbf-58fa-4dbe-b3fa-64049f009de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "yr = 2021\n",
    "#prefix = 'redtop' \n",
    "prefix = 'cper'\n",
    "#prefix = 'tbng'\n",
    "\n",
    "if prefix == 'cper': \n",
    "    aoi_f = os.path.join('data/ground/cper_pastures_2017_clip.shp')\n",
    "    df_aoi = gpd.read_file(aoi_f)\n",
    "    subunit_name_old = 'Past_Name_'\n",
    "    subunit_name = 'Pasture'\n",
    "elif prefix == 'redtop':\n",
    "    from src.utils.convert import kmz_to_shp\n",
    "    df_aoi = kmz_to_shp('data/ground/RedTop_Boundary.kmz', 'data/ground/')\n",
    "    df_aoi = df_aoi.to_crs(epsg=32613)\n",
    "    subunit_name_old = None\n",
    "    subunit_name = None\n",
    "elif prefix == 'tbng':\n",
    "    df_aoi_txt = pd.read_csv('/project/cper_neon_aop/tbng_veg/data/bm_extract/TB_all_bm.csv')\n",
    "    df_aoi = gpd.GeoDataFrame(\n",
    "        df_aoi_txt, geometry=gpd.points_from_xy(df_aoi_txt['gps_E'], df_aoi_txt['gps_N']))\n",
    "    df_aoi = df_aoi.set_crs(epsg=32613)\n",
    "    df_aoi.geometry = df_aoi.buffer(500)\n",
    "    subunit_name_old = None\n",
    "    subunit_name = None\n",
    "\n",
    "outDIR = os.path.join(outDIR_base, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b86984-eaf4-45de-a797-51f406bb1779",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wkDIR)\n",
    "print(yr)\n",
    "t00 = time.time()\n",
    "t0 = time.time()\n",
    "t1 = time.time()\n",
    "if cluster_loc == 'local':\n",
    "    print('   setting up Local cluster...')\n",
    "    from dask.distributed import LocalCluster, Client\n",
    "    import dask\n",
    "    aws=False\n",
    "    fetch.setup_env(aws=aws)\n",
    "    cluster = LocalCluster(n_workers=8, threads_per_worker=2)\n",
    "    client = Client(cluster)\n",
    "    display(client)\n",
    "elif cluster_loc == 'coiled':\n",
    "    import coiled\n",
    "    aws=True\n",
    "    fetch.setup_env(aws=aws)\n",
    "    s3_cred = fetch.setup_netrc(creds=['spkearney', '1mrChamu'], aws=aws)\n",
    "    coiled.create_software_environment(\n",
    "    name=\"hls_cog_coiled\",\n",
    "    conda=\"hls_cog_coiled_env.yaml\")\n",
    "    cluster = coiled.Cluster(\n",
    "        name=\"hls_cog_coiled\",\n",
    "        software=\"kearney-sp/hls_cog_coiled\",\n",
    "        n_workers=5,\n",
    "        worker_cpu=2,\n",
    "        scheduler_cpu=2,\n",
    "        backend_options={\"region\": \"us-west-2\"},\n",
    "        environ=dict(GDAL_DISABLE_READDIR_ON_OPEN='FALSE', \n",
    "                   #AWS_NO_SIGN_REQUEST='YES',\n",
    "                   GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "                   GDAL_SWATH_SIZE='200000000',\n",
    "                   VSI_CURL_CACHE_SIZE='200000000',\n",
    "                   CPL_VSIL_CURL_ALLOWED_EXTENSIONS='TIF',\n",
    "                   GDAL_HTTP_UNSAFESSL='YES',\n",
    "                   GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "                   GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n",
    "                   AWS_REGION='us-west-2',\n",
    "                   AWS_SECRET_ACCESS_KEY=s3_cred['secretAccessKey'],\n",
    "                   AWS_ACCESS_KEY_ID=s3_cred['accessKeyId'],\n",
    "                   AWS_SESSION_TOKEN=s3_cred['sessionToken'])\n",
    "    )\n",
    "    client = Client(cluster)\n",
    "    display(client)\n",
    "elif cluster_loc == 'hpc':\n",
    "    from dask.distributed import LocalCluster, Client\n",
    "    import dask_jobqueue as jq\n",
    "    import dask\n",
    "    from jupyter_server import serverapp\n",
    "    \n",
    "    # get the server address for porting\n",
    "    try:\n",
    "        jupServer = [x for x in serverapp.list_running_servers()][0]\n",
    "    except IndexError:\n",
    "        # manually copy/paste the server address\n",
    "        jupServer = {'base_url': '/node/ceres19-compute-98-eth.scinet.local/17710/'}\n",
    "    print('   setting up cluster on HPC...')\n",
    "    aws=False\n",
    "    fetch.setup_env(aws=aws)\n",
    "    dask.config.set({'distributed.dashboard.link': jupServer['base_url'] + 'proxy/{port}/status'})\n",
    "    partition='short',#'short','debug', 'mem', 'mem-low',\n",
    "    num_processes = 4\n",
    "    num_threads_per_processes = 2\n",
    "    mem = 2.5*num_processes*num_threads_per_processes\n",
    "    n_cores_per_job = num_processes*num_threads_per_processes\n",
    "    clust = jq.SLURMCluster(queue=partition,\n",
    "                            processes=1,\n",
    "                            #n_workers=8,\n",
    "                            cores=8,\n",
    "                            #cores=n_cores_per_job,\n",
    "                            memory=str(mem)+'6GB',\n",
    "                            interface='ib0',\n",
    "                            #interface='enp24s0f0',\n",
    "                            local_directory='$TMPDIR',\n",
    "                            death_timeout=30,\n",
    "                            walltime='02:00:00',\n",
    "                            job_extra=[\"--output=/dev/null\",\"--error=/dev/null\"])\n",
    "    client=Client(clust)\n",
    "    #Scale Cluster \n",
    "    num_jobs=64\n",
    "    clust.scale(jobs=num_jobs)\n",
    "    try:\n",
    "        client.wait_for_workers(n_workers=num_jobs*num_processes, timeout=60)\n",
    "    except dask.distributed.TimeoutError as e:\n",
    "        print(str(num_jobs*num_processes) + ' workers not available. Continuing with available workers.')\n",
    "        #print(e)\n",
    "        pass\n",
    "    display(client)\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')\n",
    "\n",
    "if not os.path.exists(outDIR):\n",
    "    os.mkdir(outDIR)\n",
    "\n",
    "if subunit_name_old is not None:\n",
    "    df_aoi = df_aoi.rename(columns={subunit_name_old: subunit_name})\n",
    "if subunit_name is not None:\n",
    "    df_aoi = df_aoi.dissolve(by=subunit_name).reset_index()\n",
    "\n",
    "start_date = str(yr - 1) + \"-11-01\"\n",
    "end_date = str(yr + 1) + \"-03-01\"\n",
    "\n",
    "# set the date range for analysis\n",
    "date_rng = pd.date_range(start=start_date, end=end_date)\n",
    "date_rng = date_rng[date_rng <= datetime.today()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761b362-b45d-406a-90fb-a56b39990c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch the COG data from the AWS bucket\n",
    "data_dict = {'date_range': [str(date_rng.min().date()), str(date_rng.max().date())]}\n",
    "hls_ds = fetch.get_hls(hls_data=data_dict,\n",
    "                       bbox=df_aoi.total_bounds, \n",
    "                       stack_chunks=(4000, 4000),\n",
    "                       proj_epsg=df_aoi.crs.to_epsg(),\n",
    "                       lim=1000,\n",
    "                       aws=aws)\n",
    "hls_ds = hls_ds.where(hls_ds['eo:cloud_cover'] != None, drop=True)\n",
    "hls_ds = hls_ds.where(hls_ds['eo:cloud_cover'] < 90, drop=True)\n",
    "hls_ds = hls_ds.sortby('time').reset_coords(drop=True).chunk({'time': -1, 'y': 100, 'x': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7798d398-84b9-45f8-8f9b-3d267220dc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e240fd5-a946-4651-a1ee-7393fed5c93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_dup_time(ds):\n",
    "    hls_mask = mask_hls(ds['FMASK'], mask_types=['all'])\n",
    "    ds['maskcov_pct'] = ((hls_mask != 0).sum(['y', 'x']) / ds['FMASK'].isel(time=0).size * 100)#\n",
    "    ds_out = ds.groupby('maskcov_pct').apply(\n",
    "        lambda x: x.sortby('maskcov_pct')).drop_duplicates(\n",
    "        'time', keep='first').drop_vars('maskcov_pct').sortby('time')\n",
    "    return ds_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1f3c75-862b-4d64-873d-e78bc6a5952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds = hls_ds.map_blocks(drop_dup_time, template=hls_ds.sortby('time').drop_duplicates('time', keep='first'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeef64da-e8dc-4002-a4ee-a993c9aa59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time to datetime\n",
    "hls_ds['time'] = pd.to_datetime(hls_ds.time)\n",
    "hls_ds = hls_ds.rename({'time': 'date'})\n",
    "# drop spec attribute for writing to NETCDF\n",
    "hls_ds.attrs = {k: hls_ds.attrs[k] for k in hls_ds.attrs if k != 'spec'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521ac54f-3e8a-4b6b-ab76-e983af4198f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5be032b-c374-4835-8806-987be5a55577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "zarr_path = os.path.join(outDIR,\n",
    "                              prefix + \n",
    "                              '_hls_tmp/hls_ds_' + \n",
    "                              str(yr) +'_' + \n",
    "                              datetime.now().strftime('%Y%m%d') + '_test.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3f869-c1d4-47eb-8770-801373b3cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(outDIR, prefix + '_hls_tmp/')):\n",
    "    os.mkdir(os.path.join(outDIR, prefix + '_hls_tmp/'))\n",
    "if os.path.exists(zarr_path):\n",
    "    shutil.rmtree(zarr_path, ignore_errors=True)\n",
    "hls_ds.to_zarr(os.path.join(zarr_path),\n",
    "              mode = 'w',\n",
    "               consolidated=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeed87d-82e9-4885-8e58-ae35f32468b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_out = xr.open_zarr(zarr_path).rename({'date': 'time'})#, chunks={'date': -1, 'y': 100, 'x': 100})\n",
    "hls_out['BLUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf19ee5-6eda-40ff-be80-d6b88e30e116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hls_funcs.masks import mask_hls, shp2mask, bolton_mask, atsa_mask\n",
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e45f4b-9bca-440e-b3ee-0b25b3944038",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_atsa = hls_out.map_blocks(atsa_mask, template=xr.ones_like(hls_out['FMASK']).to_dataset(name='ATSA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f065a6-a77b-4b09-9720-42e9a301a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub('test', 'atsa', zarr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca211e88-2e34-4e78-9db9-1433399a45cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_atsa.to_zarr(re.sub('test', 'atsa', zarr_path),\n",
    "              mode = 'w',\n",
    "               consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde1e977-2d8f-4ee6-8247-9aa7570662e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_out = xr.open_dataset('data/hls_nrt/cper/cper_hls_tmp/hls_ds_2021_20220629.nc', chunks={'date': -1, 'y': 20, 'x': 20})\n",
    "hls_out['BLUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf386f4d-de3d-4bf6-b124-2d0b3adbf4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98cbdaa-ba6b-47a8-82ec-05321cdd35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_sub = hls_out.isel(y=slice(0, 100), x=slice(0, 100), drop=True)#.chunk({'time': -1, 'y': 20, 'x': 20})\n",
    "hls_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb182ca-734f-42ca-8229-2f765983a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = atsa_mask(hls_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d3e00-552b-4a54-a815-884080d32f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94205114-6633-4ca3-a30c-d740f13e9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(da):\n",
    "    mean = da - da.mean('time')\n",
    "    return mean\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c6d8c8-3879-4899-8cac-281785e3bce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_atsa = hls_out.map_blocks(atsa_mask, template=xr.ones_like(hls_out['FMASK']).to_dataset(name='ATSA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42820718-0148-4450-a436-a897c74e579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = hls_out['BLUE'].chunk({'time': -1, 'y': 100, 'x': 100}).map_blocks(test_func,\n",
    "                                                                  template=hls_out['BLUE'].chunk({'time': -1, 'y': 100, 'x': 100}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbe89c9-f813-4b2c-8c25-48453553ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test_func(hls_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4352185-56e8-4f6c-888a-787e545b9751",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849dd223-fa65-4790-81de-388fa16cda48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8334b6a6-9cff-4e50-a5da-0be208587e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.plot import show\n",
    "show(hls_out.isel(time=100, y=slice(500, 1000), x=slice(500, 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9b444-14f8-4a15-973a-d10ccc4dae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hls_ds = hls_ds.loc[dict(x=slice(517587.0, 527283.0), y=slice(4524402.0, 4514699.0))].chunk({'y': -1,\n",
    "#                                                                                             'x': -1,\n",
    "#                                                                                             'time':1})\n",
    "#hls_mask = mask_hls(hls_ds['FMASK'])\n",
    "print('   fetching data...')\n",
    "t1 = time.time()\n",
    "idx_load_try = 0\n",
    "max_load_try = 5\n",
    "while idx_load_try < max_load_try:\n",
    "    try:\n",
    "        # pick best image for any dates with duplicate images\n",
    "        if len(np.unique(hls_ds.values)) < len(hls_ds.time.values):\n",
    "            print('    reducing along id dimension to single observation for each date, keeping least-masked image')\n",
    "            hls_ds = hls_ds.map_blocks(drop_dup_time, template=hls_ds.sortby('time').drop_duplicates('time', keep='first'))\n",
    "            \n",
    "        idx_load_try = max_load_try\n",
    "    except RuntimeError as e:            \n",
    "        if e.args[0] == 'Set changed size during iteration':\n",
    "            print('Warning: error with cluster set size. Restarting cluster and retrying ' + \\\n",
    "              str(idx_load_try+1) + ' of ' + str(max_load_try))\n",
    "            clust.close()\n",
    "            client.close()\n",
    "            clust = jq.SLURMCluster(queue=partition,\n",
    "                            processes=num_processes,\n",
    "                            cores=n_cores_per_job,\n",
    "                            memory=str(mem)+'GB',\n",
    "                            interface='ib0',\n",
    "                            #interface='enp24s0f0',\n",
    "                            local_directory='$TMPDIR',\n",
    "                            death_timeout=30,\n",
    "                            walltime='02:00:00',\n",
    "                            job_extra=[\"--output=/dev/null\",\"--error=/dev/null\"])\n",
    "            client=Client(clust)\n",
    "            #Scale Cluster \n",
    "            clust.scale(jobs=num_jobs)\n",
    "            try:\n",
    "                client.wait_for_workers(n_workers=num_jobs*num_processes, timeout=60)\n",
    "            except dask.distributed.TimeoutError as e:\n",
    "                print(str(num_jobs*num_processes) + ' workers not available. Continuing with available workers.')\n",
    "                #print(e)\n",
    "                pass\n",
    "            display(client)\n",
    "        else:\n",
    "            print('Warning: error connecting to lpdaac. Retrying ' + str(idx_load_try+1) + ' of ' + str(max_load_try))\n",
    "            client.restart()\n",
    "        idx_load_try += 1\n",
    "    except rio.errors.RasterioIOError as e:\n",
    "        print('Warning: error loading data. Retrying ' + str(idx_load_try+1) + ' of ' + str(max_load_try))\n",
    "        client.restart()\n",
    "        idx_load_try += 1\n",
    "\n",
    "#hls_ds['time'] = hls_ds['time'].dt.date\n",
    "print('...completed in ' + str(round(time.time() - t1, 0)) + ' secs')\n",
    "print('total elasped time: ' + str(round((time.time() - t0)/60, 2)) + ' mins\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38562420-ce06-4a1d-8a4a-62fc50e1bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert time to datetime\n",
    "hls_ds['time'] = pd.to_datetime(hls_ds.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2da813-dd91-4fd5-8b71-a567d7ea1dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop spec attribute for writing to NETCDF\n",
    "hls_ds.attrs = {k: hls_ds.attrs[k] for k in hls_ds.attrs if k != 'spec'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bbe0df-70e1-4e54-a7a1-6701bdb81939",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join(outDIR, prefix + '_hls_tmp/')):\n",
    "        os.mkdir(os.path.join(outDIR, prefix + '_hls_tmp/'))\n",
    "hls_ds.to_zarr(os.path.join(outDIR,\n",
    "                              prefix + \n",
    "                              '_hls_tmp/hls_ds_' + \n",
    "                              str(yr) +'_' + \n",
    "                              datetime.now().strftime('%Y%m%d') + '.zarr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23fd221-ce70-410f-b0c5-abab09770eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds.to_netcdf(os.path.join(outDIR,\n",
    "                              prefix + \n",
    "                              '_hls_tmp/hls_ds_' + \n",
    "                              str(yr) +'_' + \n",
    "                              datetime.now().strftime('%Y%m%d') + '.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de078b80-b3e9-43e5-ab90-e75fa2c2cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1adcfe4-672d-4c07-a190-106aee3a9d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
